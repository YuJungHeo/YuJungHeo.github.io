<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge; chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title> Yu-Jung Heo @ SNU </title>

<!-- Bootstrap -->
<link href="css/bootstrap.css" rel="stylesheet">
<link href="style.css" rel="stylesheet" type="text/css">
</style>
</head>

<body><center>
	<div id="wrap">
	<div id="emptytop">
	</div>
	<div id="header", style='line-height:150%'>
	
	<div class="container">	
	<div class="row">
	  <div class="col-md-4"><img class="thumbnail" src="./img/yjheo.png" style="width: 200px; height: 200px;">
	  </div>
	  <div class="col-md-8">
	  <font size='6'><b>Yu-Jung Heo</b></font><!--<a href="https://yujungheo.github.io/">[CV]</a>--><br><br>
	  Ph.D Candidate <br>
	  <a href="https://bi.snu.ac.kr">Biointelligence lab</a>, advised by Byoung-Tak Zhang<br>
	  Dept. of Computer Science and Engineering, Seoul National University <br><br>
	  
	  <b>Reseach interest: </b><br>
	  Multi-modal learning (Especially, Vision and Language), Video understanding<br>
	  <b>Email : </b>yjheo@bi.snu.ac.kr<br>
	  </div>
	</div>
	
	<div class="well">
	<b>[2022/02]</b> Our paper on knowledge-based visual question answering is accpted at ACL 2022<br>
	<b>[2021/10]</b> Our paper on video turing test is accpted at AAAI-FSS 2021<br>
	<b>[2020/12]</b> Our paper on video story understanding is accpted at AAAI 2021<br>
	<b>[2020/06]</b> Our team ranked 1st place at <a href="http://activity-net.org/challenges/2020/tasks/guest_anet_eol.html">The 1st ActivityNet Entities Object Localization challenge</a> in <a href="http://activity-net.org/challenges/2020/program.html">International Challenge on Activity Recognition (ActivityNet)</a>, CVPR 2020
	<span style="font-size:15px;">üèÜ</span><br>
	<b>[2020/03]</b> Our paper on multimodal learning is accpted at CVPR 2020<br>
	<b>[2020/02]</b> I'm co-organizing <a href="https://dramaqa.snu.ac.kr/Workshop/2020">The 2nd workshop on Video Turing Test: Toward Human-Level Video Story Understanding</a> and <a href="https://dramaqa.snu.ac.kr/Challenge/2020">the 2nd DramaQA Challenge</a> in ECCV 2020<br>

	<details>
	<summary> <b> ... Click to see more news! (until 2019) </b></summary>
        <b>[2019/12]</b> Our paper on compositional structure learning is accpted at AAAI 2020 (oral)<br>
	<b>[2019/12]</b> I'm co-organizing <a href="https://dramaqa.snu.ac.kr/Challenge/2019">The 1st DramaQA Challenge</a> in KSC 2019<br>
	<b>[2019/10]</b> I'm co-organizing <a href="https://dramaqa.snu.ac.kr/Workshop/2019">The 1st workshop on Video Turing Test: Toward Human-Level Video Story Understanding</a> in ICCV 2019<br>
	<b>[2019/06]</b> Our team ranked 1st place at <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/225/leaderboard/733">The 1st GQA challenge</a> in <a href="https://visualqa.org/workshop_2019.html">Visual Question Answering and Dialog Workshop</a> in CVPR 2019
	<span style="font-size:15px;">üèÜ</span><br>
	<b>[2019/01-2019/05]</b> I interned at Kakao Brain, Pangyo, Seongnam, Korea <br>
	<b>[2018/12]</b> Our paper on goal-oriented visual dialogue is accepted at NeurIPS 2018 (spotlight) <br>
	<b>[2018/08]</b> Our team ranked 5/312~1.6% (in-the-money) in <a href="https://www.kaggle.com/c/youtube8m-2018/leaderboard">The 2nd YouTube-8M Video Understanding challenge, ECCV 2018</a>
	</details>
		
<!-- 	<b>[2019/12]</b> Our paper on compositional structure learning is accpted at AAAI 2020 (oral)<br>
	<b>[2019/12]</b> I'm co-organizing <a href="https://dramaqa.snu.ac.kr/Challenge/2019">The 1st DramaQA Challenge</a> in KSC 2019<br>
	<b>[2019/10]</b> I'm co-organizing <a href="https://dramaqa.snu.ac.kr/Workshop/2019">The 1st workshop on Video Turing Test: Toward Human-Level Video Story Understanding</a> in ICCV 2019<br>
	<b>[2019/06]</b> Our team ranked 1st place at <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/225/leaderboard/733">The 1st GQA challenge</a> in <a href="https://visualqa.org/workshop_2019.html">Visual Question Answering and Dialog Workshop</a> in CVPR 2019<br>
	<b>[2019/01-2019/05]</b> I interned at Kakao Brain, Pangyo, Seongnam, Korea <br>
	<b>[2018/12]</b> Our paper on goal-oriented visual dialogue is accepted at NeurIPS 2018 (spotlight) <br>
	<b>[2018/08]</b> Our team ranked 5/312~1.6% (in-the-money) in <a href="https://www.kaggle.com/c/youtube8m-2018/leaderboard">The 2nd YouTube-8M Video Understanding challenge, ECCV 2018</a> -->
    </div>

	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/2204.10448" class=""><img class="thumbnail" src="./img/acl22.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering</font><br>
		<font size="1.5"><b>Yu-Jung Heo</b>, Eun-Sol Kim, Woo Suk Choi and Byoung-Tak Zhang</font><br>
		<font size="2.5"><b><i>ACL 2022</i></b><i> (acceptance ratio: 701/3378~20.75%) </i><a href="https://arxiv.org/abs/2204.10448">  [pdf]<a href="https://github.com/YuJungHeo/kbvqa-public">  [code]</a></font></div>
	</div>
		
	<div class="row">
		<span style="line-height:10%"><br><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/2110.04203" class=""><img class="thumbnail" src="./img/aaaifss21.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Toward a Human-Level Video Understanding Intelligence</font><br>
		<font size="1.5"><b>Yu-Jung Heo*</b>, Minsu Lee*, Seongho Choi, Woo Suk Choi, Minjung Shin, Minjoon Jung, Jeh-Kwang Ryu and Byoung-Tak Zhang       *Authors contributed equally</font><br>
		<font size="2.5"><b><i>AAAI 2021 Fall Symposium Series</i></b><i> on Artificial Intelligence for Human-Robot Interaction </i><a href="https://arxiv.org/abs/2110.04203">  [pdf]</a></font></div>
	</div>
		
	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/2005.03356" class=""><img class="thumbnail" src="./img/aaai21.JPG" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">DramaQA: Character-Centered Video Story Understanding with Hierarchical QA</font><br>
		<font size="1.5">Seongho Choi, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Ahjeong Seo, Youwon Jang, Minsu Lee and Byoung-Tak Zhang</font><br>
		<font size="2.5"><b><i>AAAI 2021</i></b><i> (acceptance ratio: 1692/7911~21.39%) </i><a href="https://arxiv.org/abs/2005.03356">  [pdf] <a href="https://dramaqa.snu.ac.kr/Dataset">  [dataset] <a href="https://github.com/liveseongho/DramaQA">  [code]</font></div>
		
	</div>
		
	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Hypergraph_Attention_Networks_for_Multimodal_Learning_CVPR_2020_paper.pdf" class=""><img class="thumbnail" src="./img/cvpr20.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Hypergraph Attention Networks for Multimodal Learning</font><br>
		<font size="1.5">Eun-Sol Kim*, Woo-Young Kang*, Kyoung-Woon On, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang       *Authors contributed equally</font><br>
		<font size="2.5"><b><i>CVPR 2020</i></b><i> (acceptance ratio: 1470/6656~22.09%) </i><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Hypergraph_Attention_Networks_for_Multimodal_Learning_CVPR_2020_paper.pdf">  [pdf]</a></font><br>
		<font size="2.0">‚Ä†We ranked 1/51 at the <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/225/leaderboard/733">1st GQA Challenge</a> in <a href="https://visualqa.org/workshop_2019.html">Visual Question Answering and Dialog Workshop</a> in CVPR 2019.</font>
		</div>
	</div>
		
	<div class="row">
		<span style="line-height:10%"><br><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/2001.07613" class=""><img class="thumbnail" src="./img/aaai20.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Cut-Based Graph Learning networks to Discover Compositional Structure of Sequential Video Data</font><br>
		<font size="1.5">Kyoung-Woon On, Eun-Sol Kim, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang </font><br>
		<font size="2.5"><b><i>AAAI 2020 Oral</i></b><i> (acceptance ratio: 454/7737~5.87%) </i><a href="https://arxiv.org/abs/2001.07613">  [pdf]</a></font><br>
		<font size="2.0">‚Ä†Preliminary version of this paper is presented at <i>ICML 2019¬†Workshop</i> on Learning and Reasoning with Graph-Structured Representations <a href="https://arxiv.org/abs/1907.01709.pdf">[pdf]</a> and <i>AAAI 2019¬†Workshop</i> on Network Interpretability for Deep Learning <a href="https://arxiv.org/abs/1901.09066.pdf">[pdf]</a>.</font>
		</div>
	</div>
		
	<div class="row">
		<span style="line-height:10%"><br><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/1904.00623" class=""><img class="thumbnail" src="./img/aaaisss19.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size='3'>Constructing Hierarchical Q&A Datasets for Video Story Understanding</font><br>
		<font size='1.5'><b>Yu-Jung Heo</b>, Kyoung-Woon On, Seongho Choi, Jaeseo Lim, Jinah Kim, Jeh-Kwang Ryu, Byung-Chull Bae and Byoung-Tak Zhang </font><br>
		<font size='2.5'><b><i>AAAI 2019 Spring Symposium Series</b> on Story-Enabled Intelligence</i><a href="https://arxiv.org/abs/1904.00623">  [pdf]</a></font></div>
	</div>
		
	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/1802.03881.pdf" class=""><img class="thumbnail" src="./img/nips18.jpg" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size='3'>Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog</font><br>
		<font size='1.5'>Sang-Woo Lee, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang </font><br>
		<font size='2.5'><b><i>NeurIPS 2018 Spotlight</b> (acceptance ratio: 198/4856~4.07%)</i><a href="https://arxiv.org/abs/1802.03881.pdf">  [pdf]</a></font><br>
		<font size="2.0">‚Ä†Preliminary version of this paper is presented at <i>NeurIPS 2017¬†Workshop</i> on Visually-Grounded Interaction and Language.</font></div>
	</div>

	<div class="row">
		<span style="line-height:10%"><br><br></span>
		<div class="col-md-2"><a href="https://openreview.net/pdf?id=764nB9UdW16" class=""><img class="thumbnail" src="./img/naaclws22.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Scene Graph Parsing via Abstract Meaning Representation in Pre-trained Language Models</font><br>
		<font size="1.5">Woo Suk Choi, <b>Yu-Jung Heo</b>, Dharani Punitan and Byoung-Tak Zhang </font><br>
		<font size="2.5"><b><i>NAACL 2022 Workshop</i></b><i> on Deep Learning on Graphs for Natural Language Processing (DLG4NLP)</i><a href="https://openreview.net/pdf?id=764nB9UdW16">  [pdf]</a></font></div>
	</div>

	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://www.aclweb.org/anthology/2020.alvr-1.2/" class=""><img class="thumbnail" src="./img/aclws20.JPG" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment</font><br>
		<font size="1.5">Woo Suk Choi, Kyoung-Woon On, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang </font><br>
		<font size="2.5"><b><i>ACL 2020 Workshop</i></b><i> on Advances in Language and Vision Research (ALVR)</i><a href="https://www.aclweb.org/anthology/2020.alvr-1.2/">  [pdf]</a><a href="https://github.com/cws7777/alvr-ESA">  [code]</a></font></div>
	</div>
		
<!-- 	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/1907.01709" class=""><img class="thumbnail" src="./img/icmlws19.PNG" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size='3'>Compositional Structure Learning for Sequential Video Data</font><br>
		<font size='1.5'>Kyoung-Woon On, Eun-Sol Kim, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang </font><br>
		<font size='2.5'><b><i>ICML 2019 Workshop</b> on Learning and Reasoning with Graph-Structured Representations</i><a href="https://arxiv.org/pdf/1907.01709.pdf">  [pdf]</a></font></div>
	</div> -->
		
<!-- 	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/1901.09066" class=""><img class="thumbnail" src="./img/aaaiws19.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size='3'>Visualizing Semantic Structures of Sequential Data by Learning Temporal Dependencies</font><br>
		<font size='1.5'>Kyoung-Woon On, Eun-Sol Kim, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang </font><br>
		<font size='2.5'><b><i>AAAI 2019 Workshop</b> on Network Interpretability for Deep Learning</i><a href="https://arxiv.org/abs/1901.09066.pdf">  [pdf]</a></font></div>
	</div> -->
	
	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://bi.snu.ac.kr/Publications/Conferences/International/ECCV2018_Workshop_YouTube8M_ESKim.pdf" class=""><img class="thumbnail" src="./img/eccv18ytb8m.jpg" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size='3'>Temporal Attention Mechanism with Conditional Inference for Large-scale Multi-Label Video Classification</font><br>
		<font size='1.5'>Eun-Sol Kim, Kyoung-Woon On, Jongseok Kim, <b>Yu-Jung Heo</b>, Seoungho Choi, Hyun-Dong Lee and Byoung-Tak Zhang </font><br>
		<font size='2.5'><b><i>ECCV 2018 Workshop</b> on the 2nd YouTube-8M Large-Scale Video Understanding</i><a href="https://bi.snu.ac.kr/Publications/Conferences/International/ECCV2018_Workshop_YouTube8M_ESKim.pdf">  [pdf]</a> <a href="https://static.googleusercontent.com/media/research.google.com/ko//youtube8m/workshop2018/p_c15.pdf">[slide]</a></font><br>
		<font size='2.0'>‚Ä†We ranked at 5/312~1.6% (In-the-money) in <a href="https://www.kaggle.com/c/youtube8m-2018/leaderboard">the 2nd YouTube-8M Video Understanding Challenge</a> in ECCV 2018.</font>
		</div>
	</div>
		
	<div class="row">
		<span style="line-height:10%"><br><br></span>
		<div class="col-md-2"><a href="https://bi.snu.ac.kr/~yjheo/paper/CVPRVQA_chan_yjheo.pdf" class=""><img class="thumbnail" src="./img/cvprvqa17.jpg" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size='3'>Attention Memory for Locating an Object through Visual Dialogue</font><br>
		<font size='1.5'>Cheolho Han*, <b>Yu-Jung Heo*</b>, Woo-Young Kang, Jae-Hyun Jun and Byoung-Tak Zhang       *Authors contributed equally</font><br>
		<font size='2.5'><b><i>CVPR 2017 Workshop</b> on VQA Challenge</i><a href="https://bi.snu.ac.kr/~yjheo/paper/CVPRVQA_chan_yjheo.pdf">  [pdf]</a></font></div>
	</div>
		
	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="http://ceur-ws.org/Vol-1926/paper4.pdf" class=""><img class="thumbnail" src="./img/ijcailacatoda17.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size='3'>Criteria for Human-Compatible AI in Two-Player Vision-Language Tasks </font><br>
		<font size='1.5'>Cheolho Han*, Sang-Woo Lee*, <b>Yu-Jung Heo</b>, Woo-Young Kang, Jae-Hyun Jun and Byoung-Tak Zhang       *Authors contributed equally</font><br>
		<font size='2.5'><b><i>IJCAI 2017 Workshop</b> on Linguistic and Cognitive Approaches to Dialog Agents (LaCATODA)</i><a href="http://ceur-ws.org/Vol-1926/paper4.pdf">  [pdf]</a></font></div>
	</div>
	
<!-- 	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://bi.snu.ac.kr/Publications/Conferences/International/PACS2016_YJHeo.pdf" class=""><img class="thumbnail" src="./img/pacs16.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size='3'>Pandabot: Multimodal Story Learning with Dynamic Memory Construction </font><br>
		<font size='1.5'><b>Yu-Jung Heo</b>, Eun-Sol Kim, Kyoung-Woon On and Byoung-Tak Zhang </font><br>
		<font size='2.5'><b><i>PACS 2016</b> (International Symposium on Perception, Action, and Cognitive Systems)</i><a href="https://bi.snu.ac.kr/Publications/Conferences/International/PACS2016_YJHeo.pdf">  [pdf]</a></font></div>
	</div> -->
		
	<div class="row-left">
	<h3>Domestic Journal</h3>	
		
	Efficient Compositional Translation Embedding for Visual Relationship Detection <br>
	<font size="2.5"><b>Yu-Jung Heo</b>, Eun-Sol Kim, Woo Suk Choi, Kyoung-Woon On and Byoung-Tak Zhang<br>
	<i>Journal of KIISE, Vol. 49, No. 7, Jul, 2022</i> <a href="https://doi.org/10.5626/JOK.2022.49.7.544">[pdf]</a></font><br><br>

	DramaQA: Character-Centered Video Story Understanding with Hierarchical QA <br>
	<font size="2.5">Seongho Choi, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Youwon Jang, Ahjeong Seo, Seungchan Lee, Minsu Lee and Byoung-Tak Zhang<br>
	<i>KIISE Transactions on Computer Practices, Vol. 27, No. 1, Jan, 2021</i> <a href="https://doi.org/10.5626/KTCP.2021.27.1.7">[pdf]</a></font><br><br>
		
	Analyzing and Solving GuessWhat?! <br>
	<font size='2.5'>Sang-Woo Lee, Cheolho Han, <b>Yu-Jung Heo</b>, Woo-Young Kang, Jae-Hyun Jun and Byoung-Tak Zhang<br>
	<i>Journal of KIISE, Vol. 45, No. 1, Jan, 2018</i> <a href="https://bi.snu.ac.kr/Publications/Journals/Domestic/KIISE45_1_SWLee.pdf">[pdf]</a></font><br><br>
	
	Robust Scheduling based on Daily Activity Learning by using Markov Decision Process and Inverse Reinforcement Learning <br>
	<font size='2.5'>Sang-Woo Lee, Dong-Hyun Kwak, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Woo-Young Kang, Ceyda Cinarel and Byoung-Tak Zhang<br>
	<i>KIISE Transactions on Computer Practices, Vol. 23, No. 10, Oct, 2017</i> <a href="https://bi.snu.ac.kr/Publications/Journals/Domestic/KIISE_TCP_23_10_SWLee.pdf">[pdf]</a></font><br><br>
	
	Regional Projection Histogram Matching and Linear Regression based Video Stabilization for a Moving Vehicle <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Min-Kook Choi, Hyun-Gyu Lee and Sang-Chul Lee<br>
	<i>Journal of Broadcast Engineering Vol. 19, No. 6, Nov, 2014</i> <a href="http://www.koreascience.or.kr/article/ArticleFullRecord.jsp?cn=BSGHC3_2014_v19n6_798">[pdf]</a></font><br><br>
	
	<h3>Domestic Conference</h3>
	Event Detection based on Predictive Uncertainty of User World Models <br>
	<font size="2.5"><b>Yu-Jung Heo</b>, Kibeom Kim, HoJoon Song, Hyejung Yoon and Byoung-Tak Zhang<br> 
	<i>Proc. Korea Computer Congress 2022 (KCC 2022)</i> </font><br>
	&#x2728 Award for 7 top-performing teams (announced at the ETRI human understanding AI challenge: Learning and Reasoning lifelog)<br><br>		
		
	Video Story Understanding with Multi-level Character Attention Model <br>
	<font size="2.5">Seongho Choi, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Ahjeong Seo, Youwon Jang, Minsu Lee and Byoung-Tak Zhang<br> 
	<i>Proc. Korea Computer Congress 2021 (KCC 2021)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2021/KCC2021_SHChoi.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
	
	Future State Generation for Action Prediction in Cross Domain <br>
	<font size="2.5">Hyunseo Kim, <b>Yu-Jung Heo</b>, Kibeom Kim and Byoung-Tak Zhang<br>
	<i>Proc. Korea Computer Congress 2021 (KCC 2021)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2021/KCC2021_HSKim.pdf">[pdf]</a></font><br><br>
		
	Knowledge-aware Visual Question Answering with Structural Attention Model <br>
	<font size="2.5"><b>Yu-Jung Heo</b>, Eun-Sol Kim, Woo Suk Choi and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2020 (KCC 2020)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2020/KCC2020_YJHeoKCZ.pdf">[pdf]</a></font><br><br>
	
	A study on Scene Graph Unification of Visual Semantic Knowledge using synonym <br>
	<font size="2.5">Woo Suk Choi, Kyoung-Woon on, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2020 (KCC 2020)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2020/KCC2020_WSChoiOHZ.pdf">[pdf]</a></font><br><br>
	
	A study on analysis of human and machine visual attention map for Visual Question Answering <br>
	<font size="2.5">Hyuk-Gi Lee, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2020 (KCC 2020)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2020/KCC2020_HGLeeHZ.pdf">[pdf]</a></font><br><br>
		
	DramaQA: Human Level Video Story Understanding through Multilevel Question-Answering <br>
	<font size="2.5">Seongho Choi, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Gi-Cheon Kang and Byoung-Tak Zhang <br>
	<i>Proc. Korea Software Congress 2019 (KSC 2019)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KSC2019/KSC2019_SHChoiOHKZ.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>
		
	Compositional Structure Learning for Sequential Video Data <br>
	<font size="2.5">Kyoung-Woon On, Eun-Sol Kim, <b>Yu-Jung Heo</b>, and Byoung-Tak Zhang <br>
	<i>Proc. Korea Computer Congress 2019 (KCC 2019)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2019/KCC2019_kwon.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
		
	A Study on Object Detection Technology for an Improved Visual Relationship Detection <br>
	<font size="2.5">Hyunji Choi, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2019 (KCC 2019)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2019/KCC2019_hyunjiChoi.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
		
	Analysis of Learning Strategy in AQM Framework for Goal-Oriented Visual Dialogue <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Sang-Woo Lee and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2018 (KCC 2018)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2018/KCC2018_yjheo.pdf">[pdf]</a></font><br><br>
	
	Comparison of Generative Classification Model and Discriminative Classification Model for AQM Framework <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Sang-Woo Lee and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2017 (KSC 2017)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KSC2017/KSC2017_YuJungHeo.pdf">[pdf]</a></font><br><br>
	
	Structural Knowledge Representation Learning for Content-based Question Answering <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Kyoung-Woon On, Eun-Sol Kim and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2017 (KCC 2017)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2017S/11-253.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>
	
	Analyzing and Solving GuessWhat?! <br>
	<font size='2.5'>Sang-Woo Lee, Cheolho Han, <b>Yu-Jung Heo</b>, Woo-Young Kang, Jae-Hyun Jun and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2017 (KCC 2017)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2017S/11-257.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
	
	Goal-oriented Question Generator model using Attention for GuessWhat?! <br>
	<font size='2.5'>Jae-Hyun Jun, Woo-Young Kang, Cheolho Han, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2017 (KCC 2017)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2017S/11-288.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>
	
	Adaptive Question Answering System for Personalized Language Education <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Eun-Sol Kim, Kyoung-Woon On and Byoung-Tak Zhang <br> 
	<i>Proc. Korean Institute of Intelligence Systems Spring Conference 2017 (KIIS 2017)</i> <a href="https://bi.snu.ac.kr/~yjheo/paper/KIIS2017_yjheo.pdf">[pdf]</a></font><br><br>
	
	Multimodal Story Learning with Dynamic Memory Construction <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Eun-Sol Kim, Kyoung-Woon On and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2016 (KSC 2016)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KIISE2016w_YJHeo.pdf">[pdf]</a></font><br><br>
			
	Robust Scheduling based on Daily Activity Learning by using Markov Decision Process and Inverse Reinforcement Learning <br>
	<font size='2.5'>Sang-Woo Lee, Dong-Hyun Kwak, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Woo-Young Kang, Ceyda Cinarel and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2016 (KSC 2016)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KIISE2016w_SLee.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>
		
	Dual Deep Memories for Video Question Answering <br>
	<font size='2.5'>Kyung-Min Kim, Changjun Nan, Jung-Woo Ha, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2016 (KSC 2016)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KIISE2015W_KimKM.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>

	Pororobot: A Deep Learning Robot that Plays Video Q&A Games <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Kyung-Min Kim, and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2015 (KSC 2015)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KIISE2015W_HeoYJ.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
		
	Automated Visualization Methodology for Surface of Driving road by Extracting Motion Parameters of Road Images <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Bo-Gyu Park, Hyun-Gyu Lee, Min-Kook Choi and Sang-Chul Lee <br> 
	<i>Workshop on Image Processing and Image Understanding 2015 (IPIU 2015)</i> <a href="https://bi.snu.ac.kr/~yjheo/paper/IPIU2015_yjheo.pdf">[pdf]</a></font><br><br>		
	
	Classification of Driving Events using Multi-sensor and Visualization of Driving information <br>
	<font size='2.5'>Bo-Gyu Park, <b>Yu-Jung Heo</b>, Hyun-Gyu Lee, Min-Kook Choi and Sang-Chul Lee <br> 
	<i>Workshop on Image Processing and Image Understanding 2015 (IPIU 2015)</i> <a href="https://bi.snu.ac.kr/~yjheo/paper/IPIU2015_bgpark.pdf">[pdf]</a></font><br><br>
			
	Regional Projection Histogram Matching and Linear Regression based Video Stabilization for a Moving Vehicle <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Min-Kook Choi, Hyun-Gyu Lee, and Sang-Chul Lee <br> 
	<i>Proc. Korean Institute of Broadcast and Media Engineers summer conference 2014 (KIBME 2014)</i> <a href="https://bi.snu.ac.kr/~yjheo/paper/KIBME2014_yjheo.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
	
	<em>Last update: January 2023 by Yu-Jung Heo</em>
	
	</div>
	
	<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
	</div>
</center></body>
</html>
